# Chatbot Development

## Scripts

To run the scripts in Google Colab, add `tocolab` to the URL of the notebook. For example, `https://github.com/macarious/chatbot_development/blob/main/Conversation-Agent-Inference.ipynb` becomes `https://githubtocolab.com/macarious/chatbot_development/blob/main/Conversation-Agent-Inference.ipynb`.

The following script is used to train the conversational agent with the Cornell Movie Dialogs Corpus, Web Questions Dataset, Dataset for Chatbot, and the Alpaca Dataset. The scripts are written in Python and uses the PyTorch library. The script is run in Google Colab.

1. [Conversational Agent Training Sciprt with Cornell Movie Dialogs Corpus](https://githubtocolab.com/macarious/chatbot_development/blob/main/Training/Conversation-Agent-Training-CornellMovieDialog.ipynb)

2. [Conversational Agent Training Sciprt with Web Questions Dataset](https://githubtocolab.com/macarious/chatbot_development/blob/main/Training/Conversation-Agent-Training-WebQuestions.ipynb)

3. [Conversational Agent Training Sciprt with Dataset for Chatbot](https://githubtocolab.com/macarious/chatbot_development/blob/main/Training/Conversation-Agent-Training-DatasetForChatbot.ipynb)

4. [Conversational Agent Training Sciprt with Alpaca Dataset](https://githubtocolab.com/macarious/chatbot_development/blob/main/Training/Conversation-Agent-Training-alpaca_data.ipynb)

The following script is used to infer the conversational agent with the Cornell Movie Dialogs Corpus, Web Questions Dataset, Dataset for Chatbot, and the Alpaca Dataset. The scripts are written in Python and uses the PyTorch library. The script is run in Google Colab.

1. [Conversation Agent Inference](https://githubtocolab.com/macarious/chatbot_development/blob/main/Conversation-Agent-Inference.ipynb)

## References

[1] Pasky, “Factoid webquestions dataset.” https://github.com/brmson/dataset-factoid-webquestions, 2016. [Online; accessed 13-April-2024].

[2] C. Danescu-Niculescu-Mizil and L. Lee, “Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs.,” in Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, ACL 2011, 2011.

[3] Grafstor, “Simple dialogs for chatbot.” https://www.kaggle.com/datasets/grafstor/simple-dialogs-for-chatbot, 2024.

[4] Hands on AI, “Build a chat bot from scratch using python and tensorflow.” https://handsonai.medium.com/build-a-chat-bot-from-scratch-using-python-and-tensorflow-fd189bcfae45, 2023. [Online; accessed 13-April-2024].

[5] S. Panchal, “Creating a chatbot from scratch using keras and tensorflow,” Medium, 2019. Accessed on 13-April- 2024.

[6] G. Van Rossum and F. L. Drake Jr, Python tutorial. Centrum voor Wiskunde en Informatica Amsterdam, The Netherlands, 1995.

[7] Google, “Colaboratory.” https://research.google.com/colaboratory/faq.html, 2023. [Online; accessed 13-April-2024].

[8] C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor, S. Berg, N. J. Smith, R Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk, M. Brett, A. Haldane, J. F. del R ́ıo, M. Wiebe, P. Peterson, P. G ́erard-Marchant, K. Sheppard, T. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, and T. E. Oliphant, “Array programming with NumPy,” Nature, vol. 585, pp. 357–362, Sept. 2020.

[9] M. Abadi et al., “Tensorflow: Large-scale machine learning on heterogeneous systems,” 2015. Software available from tensorflow.org.

[10] R. Rehurek and P. Sojka, “Gensim–python framework for vector space modelling,” NLP Centre, Faculty of Informatics, Masaryk University, Brno, Czech Republic, vol. 3, no. 2, 2011.

[11] OpenAI, “ChatGPT-3.5: Optimizing Language Models for Dialogue.” https://openai.com, 2022. [Online; accessed 13-April-2024].

[12] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin,S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, “Language models are few-shot learners,” 2020.

[13] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for language understanding,” 2019.
